{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpEPGNddtccp"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "This is a Google Colab Notebook which trains a PySlowFast+X3D ensemble model (using pytorchvideo) on a dataset of videos. The dataset's directory should have 3 sub-directories, one for \"train\", \"val\", and \"test\". In each of these directories should be more directories, named with the dataset's classes. Then, within each of these directories should be videos, whose label corresponds to the directory they are placed in (e.g. dataset->train->archery->archery_1.mph).\n",
        "\n",
        "The \"Set-up\" section is used for downloading necessary packages and intiliazing variables. Take a look at the variables (such as file paths), and change them accordingly.\n",
        "\n",
        "The \"Class Creation for Dataloaders\" section creates the classes/objects necessary for creating and using dataloaders later in the process. Within this section is a commented code block which would change the transform function to one of a resnet. Uncommenting this code block, and changing the model to \"make_resnet\" in \"Model Creation\" would use a resnet50 instead.\n",
        "\n",
        "The \"Model Creation\" section creates the object that is used as the Lightning Module for the model.\n",
        "\n",
        "The \"Training\" section trains the model using the Trainer from Pytorch Lightning.\n",
        "\n",
        "The \"Testing\" section tests the model using Trainer.test and a script that manually tests every file in the testing dataset.\n",
        "\n",
        "The \"TSNE\" section creates a TSNE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1yCYSsRjY5"
      },
      "source": [
        "# Set-up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP8OZwTSRk0W",
        "outputId": "775999bf-b45d-4f0e-dc4a-7053abbd4d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsUoGFOVQlw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec5fb1d-723d-4705-9d80-1ce7ef10a260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0KrecHPgICZ",
        "outputId": "5322ef9f-b387-477a-c3b8-70e51236a470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorchvideo in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (11.0.0)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.23.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (2.8.2)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.3.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorchvideo\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ykMDjE9PI-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pytorch_lightning\n",
        "import pytorchvideo.data\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import pytorchvideo\n",
        "import pytorchvideo.transforms\n",
        "from torch.nn.functional import softmax\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "import sklearn\n",
        "#from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxNxaoChPXVV"
      },
      "outputs": [],
      "source": [
        "input_dir = '/content/drive/MyDrive/Research/Muscle Video/Datasets/split_workout_videos_v1'\n",
        "model1_name = \"slowfast_r50\"\n",
        "model2_name = \"x3d_m\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Multilabel/slowfast+x3d\"\n",
        "\n",
        "model1_best_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Multilabel/slowfast/pretrained/lightning_logs/version_30/checkpoints/epoch=50-step=6528.ckpt'\n",
        "model2_best_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Multilabel/x3d_m/pretrained/lightning_logs/version_1/checkpoints/epoch=14-step=1920.ckpt'\n",
        "num_classes=16\n",
        "num_labels=11\n",
        "batch_size = 8\n",
        "num_workers = 4\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pretrained=True\n",
        "learning_rate=0.0001\n",
        "dropout_rate = 0.6\n",
        "gamma = 2\n",
        "\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "transform_params = model_transform_params[model2_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r7rgq6C8XWS",
        "outputId": "1e93717b-4e82-4015-b16d-b15cc3a47ec9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWfh_LFES6Ew"
      },
      "outputs": [],
      "source": [
        "pred_to_class = {\n",
        "    0: \"triceps\",\n",
        "    1: \"lats\",\n",
        "    2: \"biceps\",\n",
        "    3: \"quads\",\n",
        "    4: \"glutes\",\n",
        "    5: \"shoulders\",\n",
        "    6: \"abs\",\n",
        "    7: \"obliques\",\n",
        "    8: \"chest\",\n",
        "    9: \"lower back\",\n",
        "    10: \"hamstrings\",\n",
        "}\n",
        "\n",
        "id_to_exercise = {\n",
        "    0: \"bench press\",\n",
        "    1: \"bicep curl\",\n",
        "    2: \"chest fly machine\",\n",
        "    3: \"deadlift\",\n",
        "    4: \"hip thrust\",\n",
        "    5: \"lat pulling\",\n",
        "    6: \"lateral raise\",\n",
        "    7: \"leg extension\",\n",
        "    8: \"leg raises\",\n",
        "    9: \"push-up\",\n",
        "    10: \"russian twist\",\n",
        "    11: \"shoulder press\",\n",
        "    12: \"squat\",\n",
        "    13: \"t bar row\",\n",
        "    14: \"tricep Pushdown\",\n",
        "    15: \"tricep dips\",\n",
        "}\n",
        "\n",
        "class_to_label = {\n",
        "    0: [8, 5, 0],\n",
        "    1: [2],\n",
        "    2: [8],\n",
        "    3: [4, 9, 10],\n",
        "    4: [4],\n",
        "    5: [1, 2],\n",
        "    6: [5],\n",
        "    7: [3],\n",
        "    8: [6],\n",
        "    9: [8, 5, 0],\n",
        "    10: [6, 7],\n",
        "    11: [5],\n",
        "    12: [3, 4, 10],\n",
        "    13: [1, 2],\n",
        "    14: [0],\n",
        "    15: [0],\n",
        "}\n",
        "\n",
        "id_to_label = {\n",
        "    0: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    1: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    3: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
        "    4: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    5: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    6: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    7: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    9: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    10: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
        "    11: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    12: [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "    13: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    14: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    15: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POxqkd5aRrmt"
      },
      "source": [
        "# Class Creation for Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0W233scPfx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155970dc-f48f-4ef3-d44c-1a83e0495ad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom pytorchvideo.models.slowfast import create_slowfast\\n\\ndef make_slowfast():\\n    return create_slowfast(\\n        input_channels=(3, 3),\\n        model_depth=18,\\n        model_num_class=num_classes,\\n        norm=nn.BatchNorm3d,\\n        dropout_rate=dropout_rate,\\n    )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "slowfast_transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "x3d_transform = ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def post_act(input):\n",
        "  return softmax(input, dim=1)\n",
        "\n",
        "'''\n",
        "from pytorchvideo.models.slowfast import create_slowfast\n",
        "\n",
        "def make_slowfast():\n",
        "    return create_slowfast(\n",
        "        input_channels=(3, 3),\n",
        "        model_depth=18,\n",
        "        model_num_class=num_classes,\n",
        "        norm=nn.BatchNorm3d,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de65-hoGR0ES"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.utilities.combined_loader import CombinedLoader\n",
        "\n",
        "class VideosDataModule(pytorch_lightning.LightningDataModule):\n",
        "\n",
        "    # Dataset configuration\n",
        "    _DATA_PATH = input_dir\n",
        "    _CLIP_DURATION = clip_duration  # Duration of sampled clip for each video\n",
        "    _BATCH_SIZE = batch_size\n",
        "    _NUM_WORKERS = num_workers  # Number of parallel processes fetching data\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        #Create the train partition from the list of video labels and video paths\n",
        "        train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'train'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        #Create the validation partition from the list of video labels and video paths\n",
        "        val_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'val'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'test'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        test_dataloader = torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )\n",
        "\n",
        "        return test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUKUCHxR5g6"
      },
      "source": [
        "# Model Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torch.nn import Linear\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class x3d_VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.hub.load('facebookresearch/pytorchvideo', model2_name, pretrained=True)\n",
        "        self.model.to(device)\n",
        "        self.model.blocks[5].proj = Linear(in_features=2048, out_features=num_labels, bias=True)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "1zU9_XTSdW1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW0o6pPoR-w4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class slowfast_VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model1_name, pretrained=True)\n",
        "        self.model.to(device)\n",
        "        self.model.blocks[6].proj = nn.Linear(in_features=2304, out_features=num_labels, bias=True)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MultilabelAccuracy, MultilabelAUROC, MultilabelRecall, MultilabelPrecision, MultilabelF1Score\n",
        "\n",
        "mcaM2 = MultilabelAccuracy(num_labels=num_labels, average='micro')\n",
        "mcaS = MultilabelAccuracy(num_labels=num_labels, average='none')\n",
        "roc_auc = MultilabelAUROC(num_labels=num_labels, average=\"micro\")\n",
        "pre = MultilabelPrecision(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "rec = MultilabelRecall(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "f1score = MultilabelF1Score(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "\n",
        "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model1 = slowfast_VideoClassificationLightningModule.load_from_checkpoint(model1_best_path)\n",
        "        self.model1.to(device)\n",
        "        self.model1.eval()\n",
        "\n",
        "        self.model2 = x3d_VideoClassificationLightningModule.load_from_checkpoint(model2_best_path)\n",
        "        self.model2.model.blocks[5].activation = None\n",
        "        self.model2.to(device)\n",
        "        self.model2.eval()\n",
        "        #print(self.model2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "\n",
        "        slowfast_data = batch[\"video\"]\n",
        "        x3d_data = batch[\"video\"][1]\n",
        "\n",
        "        y_hat1 = self.model1(slowfast_data)\n",
        "        y_hat2 = self.model2(x3d_data)\n",
        "\n",
        "        y_hat = []\n",
        "        for x in range(0, y_hat1.shape[0]):\n",
        "          temp = []\n",
        "          for y in range(0, 11):\n",
        "            val = 0.75*y_hat1[x][y] + 0.25*y_hat2[x][y]\n",
        "            temp.append(val)\n",
        "          y_hat.append(temp)\n",
        "\n",
        "        y_hat = torch.Tensor(y_hat)\n",
        "\n",
        "        new_label = torch.tensor([id_to_label[i.item()] for i in batch[\"label\"]])\n",
        "\n",
        "        loss = F.cross_entropy(y_hat.cpu(), new_label.cpu())\n",
        "        #loss = torch.Tensor(0)\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        accM2 = mcaM2(y_hat.cpu(), new_label.cpu())\n",
        "        self.log(\"test_accuracy_micro\", accM2.item(), batch_size=batch_size)\n",
        "\n",
        "        new_label = new_label.type(torch.IntTensor)\n",
        "\n",
        "        auc = roc_auc(y_hat.cpu(), new_label.cpu())\n",
        "        self.log(\"auc\", auc.item(), batch_size=batch_size)\n",
        "\n",
        "        precision = pre(y_hat.cpu(), new_label.cpu())\n",
        "        recall = rec(y_hat.cpu(), new_label.cpu())\n",
        "        f1 = f1score(y_hat.cpu(), new_label.cpu())\n",
        "\n",
        "        self.log(\"precision\", precision.item(), batch_size=batch_size)\n",
        "        self.log(\"recall\", recall.item(), batch_size=batch_size)\n",
        "        self.log(\"f1\", f1.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_micro:\", accM2.item(), \"auc:\", auc.item(), \"precision:\", precision.item(), \"recall\", recall.item(), \"f1\", f1.item())\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "uB9TZKyoeGMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5dqGI2sSaZz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggfgr6aSww4a"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping_callbacks = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10, verbose=True, mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yBLmhEQTLbG",
        "outputId": "99a68529-2cab-4682-859a-2f537cd08ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "classification_module = VideoClassificationLightningModule()\n",
        "data_module = VideosDataModule()\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    default_root_dir=checkpoint_path,\n",
        "    max_epochs=30,\n",
        "    accelerator=\"auto\",\n",
        "    devices=\"auto\",\n",
        "    strategy='auto',\n",
        "    enable_checkpointing=True,\n",
        "    logger=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEXoz1KX7_C7"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldyTiRKsvSyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "a23e9735-3345-41a0-a720-bc02e5092b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-443819fa368d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: configuring model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# restore modules after setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore_training_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoClassificationLightningModule:\n\tUnexpected key(s) in state_dict: \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_block...\n\tsize mismatch for model.blocks.0.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([16, 8, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 8, 7, 1, 1]).\n\tsize mismatch for model.blocks.1.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([64, 32, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 7, 1, 1]).\n\tsize mismatch for model.blocks.2.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([128, 64, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 7, 1, 1]).\n\tsize mismatch for model.blocks.3.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([256, 128, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 7, 1, 1])."
          ]
        }
      ],
      "source": [
        "trainer.fit(classification_module, data_module, ckpt_path='/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csUFcsFoKft"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_QYVE-AUbhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "949d0729d39e43339a48a76694ffa5af",
            "d91b6b12719841798691b9250cb6ada6",
            "25fa8907e7054eeeae731e1b286cdf33",
            "d87fcb10c278452a93477b204a9a8500",
            "0ce6fc7817bc4c0bbb6eb95f05ee491a",
            "e1f596b1762b44e1b04180ac436f6c7d",
            "b8ae0cb78c0c429eb980ea43f1fe8c66",
            "a9bdf6c309784095b47f05b4426a567d",
            "826334b905434bea89f58603c2ebf141",
            "9d36c9a7f57c48c7981ab2e7e0f24440",
            "abeb113a18ea485f88fd070cabbbc1e7"
          ]
        },
        "outputId": "168b235c-810f-47a6-fbd4-c13313aad0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "949d0729d39e43339a48a76694ffa5af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 1.962888479232788 test_accuracy_micro: 0.9545454382896423 auc: 0.9951738119125366 precision: 0.8030303716659546 recall 0.9090909957885742 f1 0.8333333730697632\n",
            "test_loss: 1.4779531955718994 test_accuracy_micro: 0.9659090638160706 auc: 0.9942084550857544 precision: 0.5833333134651184 recall 0.5909091234207153 f1 0.5748917460441589\n",
            "test_loss: 2.45849347114563 test_accuracy_micro: 0.9659090638160706 auc: 0.9861111640930176 precision: 0.7272727489471436 recall 0.6212121248245239 f1 0.6606060266494751\n",
            "test_loss: 1.699234127998352 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 1.5587481260299683 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 1.3723828792572021 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.41363635659217834 recall 0.4545454680919647 f1 0.43145743012428284\n",
            "test_loss: 1.7734178304672241 test_accuracy_micro: 0.9772727489471436 auc: 0.9973957538604736 precision: 0.6060606241226196 recall 0.6363636255264282 f1 0.6181818246841431\n",
            "test_loss: 2.883075714111328 test_accuracy_micro: 0.9886363744735718 auc: 0.9992647171020508 precision: 0.7878788709640503 recall 0.8181818723678589 f1 0.8000000715255737\n",
            "test_loss: 2.120114803314209 test_accuracy_micro: 1.0 auc: 0.9999999403953552 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 2.0947952270507812 test_accuracy_micro: 0.9772727489471436 auc: 0.9961389899253845 precision: 0.8636364936828613 recall 0.8787878751754761 f1 0.8606060743331909\n",
            "test_loss: 3.40411114692688 test_accuracy_micro: 0.9659090638160706 auc: 0.9926470518112183 precision: 0.7045454978942871 recall 0.7272727489471436 f1 0.7142857313156128\n",
            "test_loss: 2.1707842350006104 test_accuracy_micro: 0.9886363744735718 auc: 1.0 precision: 0.6818182468414307 recall 0.7272727489471436 f1 0.696969747543335\n",
            "test_loss: 7.586857318878174 test_accuracy_micro: 0.9545454382896423 auc: 0.9351640939712524 precision: 0.7272727489471436 recall 0.6060606241226196 f1 0.6484848260879517\n",
            "test_loss: 0.587663471698761 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 0.9308433532714844 test_accuracy_micro: 0.9659090638160706 auc: 0.9987180233001709 precision: 0.5 recall 0.5454545617103577 f1 0.5151515007019043\n",
            "test_loss: 2.3084213733673096 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.8333334922790527 recall 0.9090909957885742 f1 0.8606061935424805\n",
            "test_loss: 2.452042818069458 test_accuracy_micro: 0.9318181872367859 auc: 0.9835616946220398 precision: 0.4545454680919647 recall 0.5454545617103577 f1 0.4848484992980957\n",
            "test_loss: 1.573378562927246 test_accuracy_micro: 0.9886363744735718 auc: 1.0 precision: 0.772727370262146 recall 0.8181818723678589 f1 0.7878788709640503\n",
            "test_loss: 1.9018607139587402 test_accuracy_micro: 0.9772727489471436 auc: 0.9981735348701477 precision: 0.7545454502105713 recall 0.8181818723678589 f1 0.777777910232544\n",
            "test_loss: 2.450099229812622 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 2.5000810623168945 test_accuracy_micro: 0.9545454382896423 auc: 0.9933719635009766 precision: 0.8181818723678589 recall 0.8181819915771484 f1 0.7878788709640503\n",
            "test_loss: 0.07662913203239441 test_accuracy_micro: 0.9886363744735718 auc: 1.0 precision: 0.3636363744735718 recall 0.3636363744735718 f1 0.3636363744735718\n",
            "test_loss: 1.6959154605865479 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 2.0750555992126465 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.575757622718811 recall 0.6363636255264282 f1 0.6000000238418579\n",
            "test_loss: 1.886061668395996 test_accuracy_micro: 0.9431818127632141 auc: 0.9845559597015381 precision: 0.6060606241226196 recall 0.6742424964904785 f1 0.6173160076141357\n",
            "test_loss: 1.7182241678237915 test_accuracy_micro: 0.9886363744735718 auc: 0.9991319179534912 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 1.4730055332183838 test_accuracy_micro: 0.9545454382896423 auc: 1.0 precision: 0.7272727489471436 recall 0.9090909957885742 f1 0.7878788709640503\n",
            "test_loss: 1.6501388549804688 test_accuracy_micro: 1.0 auc: 0.9999999403953552 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 1.950291633605957 test_accuracy_micro: 0.9659090638160706 auc: 0.9844748973846436 precision: 0.5909091234207153 recall 0.5606061220169067 f1 0.5727273225784302\n",
            "test_loss: 2.627530813217163 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 2.5900213718414307 test_accuracy_micro: 0.9772727489471436 auc: 0.9969488978385925 precision: 0.6515151262283325 recall 0.7272727489471436 f1 0.6787879467010498\n",
            "test_loss: 2.6561458110809326 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 3.0843307971954346 test_accuracy_micro: 0.9318181872367859 auc: 0.9801160097122192 precision: 0.6818182468414307 recall 0.772727370262146 f1 0.7121212482452393\n",
            "test_loss: 2.2647199630737305 test_accuracy_micro: 0.9772727489471436 auc: 0.9982638359069824 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 0.7663600444793701 test_accuracy_micro: 0.9886363744735718 auc: 1.0 precision: 0.6060606241226196 recall 0.6363636255264282 f1 0.6181818246841431\n",
            "test_loss: 1.2014092206954956 test_accuracy_micro: 0.9886363744735718 auc: 0.9969230890274048 precision: 0.7878788709640503 recall 0.8181818723678589 f1 0.8000000715255737\n",
            "test_loss: 1.3105568885803223 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.5681818723678589 recall 0.6363636255264282 f1 0.5930736064910889\n",
            "test_loss: 0.9871236681938171 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 1.394368052482605 test_accuracy_micro: 0.9772727489471436 auc: 1.0 precision: 0.8333333730697632 recall 0.9090909957885742 f1 0.8606060743331909\n",
            "test_loss: 2.7566888332366943 test_accuracy_micro: 0.9545454382896423 auc: 0.9968253970146179 precision: 0.6818182468414307 recall 0.696969747543335 f1 0.6787879467010498\n",
            "test_loss: 4.351203918457031 test_accuracy_micro: 0.9090909361839294 auc: 0.9175823926925659 precision: 0.40909093618392944 recall 0.3636363744735718 f1 0.3636363744735718\n",
            "test_loss: 3.1411330699920654 test_accuracy_micro: 0.8787878751754761 auc: 0.8793103098869324 precision: 0.1818181872367859 recall 0.13636364042758942 f1 0.1515151560306549\n",
            "test_loss: 1.6553692817687988 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.27272728085517883 recall 0.27272728085517883 f1 0.27272728085517883\n",
            "test_loss: 1.6804327964782715 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.3636363744735718 recall 0.3636363744735718 f1 0.3636363744735718\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m           auc           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9910013675689697    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m           f1            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6528170704841614    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6493113040924072    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         recall          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6713154911994934    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   test_accuracy_micro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9742595553398132    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0968172550201416    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            auc            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9910013675689697     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            f1             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6528170704841614     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6493113040924072     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          recall           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6713154911994934     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_micro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9742595553398132     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0968172550201416     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 2.0968172550201416,\n",
              "  'test_accuracy_micro': 0.9742595553398132,\n",
              "  'auc': 0.9910013675689697,\n",
              "  'precision': 0.6493113040924072,\n",
              "  'recall': 0.6713154911994934,\n",
              "  'f1': 0.6528170704841614}]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "trainer.test(model=classification_module, datamodule=data_module, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TSNE"
      ],
      "metadata": {
        "id": "BtFuX8SuCGr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TSNE_model = VideoClassificationLightningModule.load_from_checkpoint(best_checkpoint_path)\n",
        "TSNE_model.to('cpu')\n",
        "TSNE_model.model.blocks.pop(6)\n",
        "TSNE_model.eval()\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULEPmSyTCILk",
        "outputId": "47170f91-3af9-4acb-af3c-456b76bc26b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RbmJ1rHPCLHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 10\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "7myJZnrNCMJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "features = None"
      ],
      "metadata": {
        "id": "W_oirFUNCOJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thingy = data_module.test_dataloader()\n",
        "for batch in thingy:\n",
        "  x = batch[\"video\"]\n",
        "  labels += batch[\"label\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = TSNE_model(x)\n",
        "    output = torch.flatten(output, 1)\n",
        "\n",
        "  current_features = output.cpu().numpy()\n",
        "\n",
        "  if features is not None:\n",
        "    features = np.concatenate((features, current_features))\n",
        "  else:\n",
        "    features = current_features"
      ],
      "metadata": {
        "id": "zLsYVEWECPrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2).fit_transform(features)"
      ],
      "metadata": {
        "id": "RnJeizXECSW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale and move the coordinates so they fit [0; 1] range\n",
        "def scale_to_01_range(x):\n",
        "    # compute the distribution range\n",
        "    value_range = (np.max(x) - np.min(x))\n",
        "\n",
        "    # move the distribution so that it starts from zero\n",
        "    # by extracting the minimal value from all its values\n",
        "    starts_from_zero = x - np.min(x)\n",
        "\n",
        "    # make the distribution fit [0; 1] by dividing by its range\n",
        "    return starts_from_zero / value_range\n",
        "\n",
        "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
        "tx = tsne[:, 0]\n",
        "ty = tsne[:, 1]\n",
        "\n",
        "tx = scale_to_01_range(tx)\n",
        "ty = scale_to_01_range(ty)"
      ],
      "metadata": {
        "id": "5PXEvxJlCUJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors_per_class = {\n",
        "    0: [254, 202, 87],\n",
        "    1: [255, 107, 107],\n",
        "    2: [10, 189, 227],\n",
        "    3: [255, 159, 243],\n",
        "    4: [16, 172, 132],\n",
        "    5: [128, 80, 128],\n",
        "    6: [87, 101, 116],\n",
        "    7: [52, 31, 151],\n",
        "    8: [0, 0, 0],\n",
        "    9: [100, 100, 255],\n",
        "    10: [128, 109, 84],\n",
        "    11: [15, 245, 218],\n",
        "    12: [242, 10, 21],\n",
        "    13: [242, 10, 149],\n",
        "    14: [40, 242, 0],\n",
        "    15: [255, 153, 10]\n",
        "}"
      ],
      "metadata": {
        "id": "QVF3CvuOCVZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a matplotlib plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# for every class, we'll add a scatter plot separately\n",
        "for label in colors_per_class:\n",
        "    # find the samples of the current class in the data\n",
        "    indices = [i for i, l in enumerate(labels) if l == label]\n",
        "\n",
        "    # extract the coordinates of the points of this class only\n",
        "    current_tx = np.take(tx, indices)\n",
        "    current_ty = np.take(ty, indices)\n",
        "\n",
        "    # convert the class color to matplotlib format\n",
        "    color = np.array(colors_per_class[label], dtype=np.float) / 255\n",
        "\n",
        "    # add a scatter plot with the corresponding color and label\n",
        "    ax.scatter(current_tx, current_ty, c=color, label=label, s=15)\n",
        "\n",
        "# build a legend using the labels we set previously\n",
        "ax.legend(loc='best')\n",
        "\n",
        "# finally, show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "7rIVvzH-CWkq",
        "outputId": "a1edde73-53a9-489b-b8aa-d196f222e643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-885392fecd81>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  color = np.array(colors_per_class[label], dtype=np.float) / 255\n",
            "<ipython-input-45-885392fecd81>:18: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  ax.scatter(current_tx, current_ty, c=color, label=label, s=15)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zF1yCYSsRjY5",
        "POxqkd5aRrmt",
        "BtFuX8SuCGr-"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}