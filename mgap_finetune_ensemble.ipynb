{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpEPGNddtccp"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "This is a Google Colab Notebook which trains a PySlowFast+X3D ensemble model (using pytorchvideo) on a dataset of videos. The dataset's directory should have 3 sub-directories, one for \"train\", \"val\", and \"test\". In each of these directories should be more directories, named with the dataset's classes. Then, within each of these directories should be videos, whose label corresponds to the directory they are placed in (e.g. dataset->train->archery->archery_1.mph).\n",
        "\n",
        "The \"Set-up\" section is used for downloading necessary packages and intiliazing variables. Take a look at the variables (such as file paths), and change them accordingly.\n",
        "\n",
        "The \"Class Creation for Dataloaders\" section creates the classes/objects necessary for creating and using dataloaders later in the process. Within this section is a commented code block which would change the transform function to one of a resnet. Uncommenting this code block, and changing the model to \"make_resnet\" in \"Model Creation\" would use a resnet50 instead.\n",
        "\n",
        "The \"Model Creation\" section creates the object that is used as the Lightning Module for the model.\n",
        "\n",
        "The \"Training\" section trains the model using the Trainer from Pytorch Lightning.\n",
        "\n",
        "The \"Testing\" section tests the model using Trainer.test as well as a script that manually tests every file in the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1yCYSsRjY5"
      },
      "source": [
        "# Set-up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP8OZwTSRk0W",
        "outputId": "92c06a65-b544-4d0f-b11c-cd872c518055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsUoGFOVQlw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cbaf19-2468-4cec-8e88-dc647e4cd149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0KrecHPgICZ",
        "outputId": "7c422637-ba08-425c-d58c-f107f881f415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore (from pytorchvideo)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av (from pytorchvideo)\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting iopath (from pytorchvideo)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.5.0)\n",
            "Collecting portalocker (from iopath->pytorchvideo)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=36b51648007e9840aefda12000a88e0df856bb7474fdc7ac6c256248376c35e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=0f00dea1bc62e0a2524f543d03746e5e8e846ccc91507ee48f8c2bb465cd6dd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=1eda3e05afeb3524e02b15d86010052a25fc19cad0c33d2def0d691a0063fe75\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-11.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.8.2 pytorchvideo-0.1.5 yacs-0.1.8\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.10.1 pytorch_lightning-2.1.4 torchmetrics-1.3.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorchvideo\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ykMDjE9PI-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c90214-a7cc-473a-a842-f0f326a5f22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pytorch_lightning\n",
        "import pytorchvideo.data\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import pytorchvideo\n",
        "import pytorchvideo.transforms\n",
        "from torch.nn.functional import softmax\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "import sklearn\n",
        "#from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxNxaoChPXVV"
      },
      "outputs": [],
      "source": [
        "input_dir = '/content/drive/MyDrive/Research/Muscle Video/Datasets/split_workout_videos_v1'\n",
        "model1_name = \"slowfast_r50\"\n",
        "model2_name = \"x3d_m\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/double_backbone/slowfast+x3d\"\n",
        "\n",
        "model1_best_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Multilabel/slowfast/backbone from exercise/lightning_logs/version_5/checkpoints/epoch=29-step=3854.ckpt'\n",
        "model2_best_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Multilabel/x3d_m/backbone/lightning_logs/version_0/checkpoints/epoch=29-step=3900.ckpt'\n",
        "num_classes=16\n",
        "num_labels=11\n",
        "batch_size = 8\n",
        "num_workers = 4\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pretrained=True\n",
        "learning_rate=0.0001\n",
        "dropout_rate = 0.6\n",
        "gamma = 2\n",
        "\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "transform_params = model_transform_params[model2_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r7rgq6C8XWS",
        "outputId": "31e2193a-2ebe-4f92-87ea-6b6d6c09bcaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWfh_LFES6Ew"
      },
      "outputs": [],
      "source": [
        "pred_to_class = {\n",
        "    0: \"triceps\",\n",
        "    1: \"lats\",\n",
        "    2: \"biceps\",\n",
        "    3: \"quads\",\n",
        "    4: \"glutes\",\n",
        "    5: \"shoulders\",\n",
        "    6: \"abs\",\n",
        "    7: \"obliques\",\n",
        "    8: \"chest\",\n",
        "    9: \"lower back\",\n",
        "    10: \"hamstrings\",\n",
        "}\n",
        "\n",
        "id_to_exercise = {\n",
        "    0: \"bench press\",\n",
        "    1: \"bicep curl\",\n",
        "    2: \"chest fly machine\",\n",
        "    3: \"deadlift\",\n",
        "    4: \"hip thrust\",\n",
        "    5: \"lat pulling\",\n",
        "    6: \"lateral raise\",\n",
        "    7: \"leg extension\",\n",
        "    8: \"leg raises\",\n",
        "    9: \"push-up\",\n",
        "    10: \"russian twist\",\n",
        "    11: \"shoulder press\",\n",
        "    12: \"squat\",\n",
        "    13: \"t bar row\",\n",
        "    14: \"tricep Pushdown\",\n",
        "    15: \"tricep dips\",\n",
        "}\n",
        "\n",
        "class_to_label = {\n",
        "    0: [8, 5, 0],\n",
        "    1: [2],\n",
        "    2: [8],\n",
        "    3: [4, 9, 10],\n",
        "    4: [4],\n",
        "    5: [1, 2],\n",
        "    6: [5],\n",
        "    7: [3],\n",
        "    8: [6],\n",
        "    9: [8, 5, 0],\n",
        "    10: [6, 7],\n",
        "    11: [5],\n",
        "    12: [3, 4, 10],\n",
        "    13: [1, 2],\n",
        "    14: [0],\n",
        "    15: [0],\n",
        "}\n",
        "\n",
        "id_to_label = {\n",
        "    0: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    1: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    3: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
        "    4: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    5: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    6: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    7: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    9: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    10: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
        "    11: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    12: [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "    13: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    14: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    15: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POxqkd5aRrmt"
      },
      "source": [
        "# Class Creation for Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0W233scPfx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baea7f16-54ed-4ccf-c267-d240a90aec48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom pytorchvideo.models.slowfast import create_slowfast\\n\\ndef make_slowfast():\\n    return create_slowfast(\\n        input_channels=(3, 3),\\n        model_depth=18,\\n        model_num_class=num_classes,\\n        norm=nn.BatchNorm3d,\\n        dropout_rate=dropout_rate,\\n    )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "slowfast_transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "x3d_transform = ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def post_act(input):\n",
        "  return softmax(input, dim=1)\n",
        "\n",
        "'''\n",
        "from pytorchvideo.models.slowfast import create_slowfast\n",
        "\n",
        "def make_slowfast():\n",
        "    return create_slowfast(\n",
        "        input_channels=(3, 3),\n",
        "        model_depth=18,\n",
        "        model_num_class=num_classes,\n",
        "        norm=nn.BatchNorm3d,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de65-hoGR0ES"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.utilities.combined_loader import CombinedLoader\n",
        "\n",
        "class VideosDataModule(pytorch_lightning.LightningDataModule):\n",
        "\n",
        "    # Dataset configuration\n",
        "    _DATA_PATH = input_dir\n",
        "    _CLIP_DURATION = clip_duration  # Duration of sampled clip for each video\n",
        "    _BATCH_SIZE = batch_size\n",
        "    _NUM_WORKERS = num_workers  # Number of parallel processes fetching data\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        #Create the train partition from the list of video labels and video paths\n",
        "        train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'train'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        #Create the validation partition from the list of video labels and video paths\n",
        "        val_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'val'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'test'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=slowfast_transform\n",
        "        )\n",
        "\n",
        "        test_dataloader = torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )\n",
        "\n",
        "        return test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUKUCHxR5g6"
      },
      "source": [
        "# Model Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torch.nn import Linear\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class OG_X3D_VideoClassificationModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.hub.load('facebookresearch/pytorchvideo', model2_name, pretrained=True)\n",
        "        self.model.to(device)\n",
        "        self.model.blocks[5].proj = Linear(in_features=2048, out_features=num_classes, bias=True)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "x3d_backbone_checkpoint = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/x3d/x3d_m/lightning_logs/version_3/checkpoints/epoch=29-step=3868.ckpt'"
      ],
      "metadata": {
        "id": "IQb9jrLwZ4ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torch.nn import Linear\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class x3d_VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = OG_X3D_VideoClassificationModule.load_from_checkpoint(x3d_backbone_checkpoint)\n",
        "        self.model.to(device)\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        self.model.model.blocks[5].proj = nn.Linear(in_features=2048, out_features=11, bias=True)\n",
        "\n",
        "        for param in self.model.model.blocks[5].proj.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "1zU9_XTSdW1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class OG_VideoClassificationModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model1_name, pretrained=True)\n",
        "        self.model.to(device)\n",
        "        self.model.blocks[6].proj = nn.Linear(in_features=2304, out_features=16, bias=True)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "backbone_checkpoint = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r50/lightning_logs/version_4/checkpoints/epoch=4-step=640.ckpt'"
      ],
      "metadata": {
        "id": "I2DZIRc4P-lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW0o6pPoR-w4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class slowfast_VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = OG_VideoClassificationModule.load_from_checkpoint(backbone_checkpoint)\n",
        "        self.model.to(device)\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        self.model.model.blocks[6].proj = nn.Linear(in_features=2304, out_features=11, bias=True)\n",
        "\n",
        "        for param in self.model.model.blocks[6].proj.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MultilabelAccuracy, MultilabelAUROC, MultilabelRecall, MultilabelPrecision, MultilabelF1Score\n",
        "\n",
        "mcaM2 = MultilabelAccuracy(num_labels=num_labels, average='micro')\n",
        "mcaS = MultilabelAccuracy(num_labels=num_labels, average='none')\n",
        "roc_auc = MultilabelAUROC(num_labels=num_labels, average=\"micro\")\n",
        "pre = MultilabelPrecision(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "rec = MultilabelRecall(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "f1score = MultilabelF1Score(num_labels=num_labels, average='macro', threshold=0.5)\n",
        "\n",
        "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model1 = slowfast_VideoClassificationLightningModule.load_from_checkpoint(model1_best_path)\n",
        "        self.model1.to(device)\n",
        "        self.model1.eval()\n",
        "\n",
        "        self.model2 = x3d_VideoClassificationLightningModule.load_from_checkpoint(model2_best_path)\n",
        "        self.model2.model.model.blocks[5].activation = None\n",
        "        self.model2.to(device)\n",
        "        self.model2.eval()\n",
        "        #print(self.model2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "\n",
        "        slowfast_data = batch[\"video\"]\n",
        "        x3d_data = batch[\"video\"][1]\n",
        "\n",
        "        y_hat1 = self.model1(slowfast_data)\n",
        "        y_hat2 = self.model2(x3d_data)\n",
        "\n",
        "        y_hat = []\n",
        "        for x in range(0, y_hat1.shape[0]):\n",
        "          temp = []\n",
        "          for y in range(0, 11):\n",
        "            val = 0.75*y_hat1[x][y] + 0.25*y_hat2[x][y]\n",
        "            temp.append(val)\n",
        "          y_hat.append(temp)\n",
        "\n",
        "        y_hat = torch.Tensor(y_hat)\n",
        "\n",
        "        new_label = torch.tensor([id_to_label[i.item()] for i in batch[\"label\"]])\n",
        "\n",
        "        loss = F.cross_entropy(y_hat.cpu(), new_label.cpu())\n",
        "        #loss = torch.Tensor(0)\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        accM2 = mcaM2(y_hat.cpu(), new_label.cpu())\n",
        "        self.log(\"test_accuracy_micro\", accM2.item(), batch_size=batch_size)\n",
        "\n",
        "        new_label = new_label.type(torch.IntTensor)\n",
        "\n",
        "        auc = roc_auc(y_hat.cpu(), new_label.cpu())\n",
        "        self.log(\"auc\", auc.item(), batch_size=batch_size)\n",
        "\n",
        "        precision = pre(y_hat.cpu(), new_label.cpu())\n",
        "        recall = rec(y_hat.cpu(), new_label.cpu())\n",
        "        f1 = f1score(y_hat.cpu(), new_label.cpu())\n",
        "\n",
        "        self.log(\"precision\", precision.item(), batch_size=batch_size)\n",
        "        self.log(\"recall\", recall.item(), batch_size=batch_size)\n",
        "        self.log(\"f1\", f1.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_micro:\", accM2.item(), \"auc:\", auc.item(), \"precision:\", precision.item(), \"recall\", recall.item(), \"f1\", f1.item())\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "uB9TZKyoeGMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5dqGI2sSaZz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggfgr6aSww4a"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping_callbacks = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10, verbose=True, mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yBLmhEQTLbG",
        "outputId": "95783078-5c92-4451-df31-f58f317ac54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "classification_module = VideoClassificationLightningModule()\n",
        "data_module = VideosDataModule()\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    default_root_dir=checkpoint_path,\n",
        "    max_epochs=30,\n",
        "    accelerator=\"auto\",\n",
        "    devices=\"auto\",\n",
        "    strategy='auto',\n",
        "    enable_checkpointing=True,\n",
        "    logger=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEXoz1KX7_C7"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldyTiRKsvSyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "a23e9735-3345-41a0-a720-bc02e5092b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-443819fa368d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: configuring model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# restore modules after setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore_training_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoClassificationLightningModule:\n\tUnexpected key(s) in state_dict: \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_block...\n\tsize mismatch for model.blocks.0.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([16, 8, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 8, 7, 1, 1]).\n\tsize mismatch for model.blocks.1.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([64, 32, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 7, 1, 1]).\n\tsize mismatch for model.blocks.2.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([128, 64, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 7, 1, 1]).\n\tsize mismatch for model.blocks.3.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([256, 128, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 7, 1, 1])."
          ]
        }
      ],
      "source": [
        "trainer.fit(classification_module, data_module, ckpt_path='/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csUFcsFoKft"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_QYVE-AUbhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9eec9e9f1996457e9f20ef6a937f0813",
            "68ca154fd0e64be89deafcd1b9a58d51",
            "3ef3e329ac484f508d7d6768337d9f0a",
            "93839f48bec34db887962c1a7cdb0035",
            "fee7f2d5041e4ac2b389396275913ede",
            "90063137eb8f4af89dd3759a62929d6a",
            "521c802bce4740a38eea286d87e74087",
            "dc8079d9f0fe45c890979a9d7c506cb7",
            "44960fd5b0784b7696e3de0aa4f2ca1f",
            "7d43b15a5c3040cfa49ba6fc2fb938e7",
            "404856f5f586439fa7ae55ffef553012"
          ]
        },
        "outputId": "a77fac33-1f92-4b3d-845d-07efe5b79026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eec9e9f1996457e9f20ef6a937f0813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 2.013576030731201 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 2.1450650691986084 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 0.9566322565078735 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 3.4235424995422363 test_accuracy_micro: 0.9318181872367859 auc: 0.9611110687255859 precision: 0.6818182468414307 recall 0.575757622718811 f1 0.6151515245437622\n",
            "test_loss: 1.8996312618255615 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.9090909957885742 recall 0.9090909957885742 f1 0.9090909957885742\n",
            "test_loss: 0.9732237458229065 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 1.738102674484253 test_accuracy_micro: 1.0 auc: 0.9999999403953552 precision: 0.5454545617103577 recall 0.5454545617103577 f1 0.5454545617103577\n",
            "test_loss: 2.396064519882202 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 1.727573037147522 test_accuracy_micro: 0.9772727489471436 auc: 0.9913127422332764 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 1.2937085628509521 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.3636363744735718 recall 0.3636363744735718 f1 0.3636363744735718\n",
            "test_loss: 1.763717532157898 test_accuracy_micro: 0.9772727489471436 auc: 0.9936072826385498 precision: 0.7272727489471436 recall 0.6818182468414307 f1 0.696969747543335\n",
            "test_loss: 1.516562581062317 test_accuracy_micro: 0.9772727489471436 auc: 0.9917949438095093 precision: 0.6363636255264282 recall 0.5909091234207153 f1 0.6060606241226196\n",
            "test_loss: 2.420022487640381 test_accuracy_micro: 0.9772727489471436 auc: 0.9908864498138428 precision: 0.6363636255264282 recall 0.5909091234207153 f1 0.6060606241226196\n",
            "test_loss: 3.171267032623291 test_accuracy_micro: 0.9318181872367859 auc: 0.9555555582046509 precision: 0.7272727489471436 recall 0.6060606241226196 f1 0.6363636255264282\n",
            "test_loss: 1.4349654912948608 test_accuracy_micro: 0.9659090638160706 auc: 0.9928205013275146 precision: 0.7272727489471436 recall 0.5909091234207153 f1 0.6363636255264282\n",
            "test_loss: 1.9390156269073486 test_accuracy_micro: 0.9772727489471436 auc: 0.9890410900115967 precision: 0.8181818723678589 recall 0.7272727489471436 f1 0.7575758695602417\n",
            "test_loss: 1.372875690460205 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.9090909957885742 recall 0.9090909957885742 f1 0.9090909957885742\n",
            "test_loss: 1.4223424196243286 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 2.5965144634246826 test_accuracy_micro: 0.9318181872367859 auc: 0.9792875051498413 precision: 0.4545454680919647 recall 0.4545454680919647 f1 0.4545454680919647\n",
            "test_loss: 1.8612371683120728 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.4545454680919647 recall 0.4545454680919647 f1 0.4545454680919647\n",
            "test_loss: 1.7691664695739746 test_accuracy_micro: 0.9431818127632141 auc: 0.9714912176132202 precision: 0.5454545617103577 recall 0.3333333730697632 f1 0.4060606062412262\n",
            "test_loss: 1.8549168109893799 test_accuracy_micro: 0.9431818127632141 auc: 0.9936073422431946 precision: 0.7272727489471436 recall 0.5909091234207153 f1 0.6363636255264282\n",
            "test_loss: 1.927072286605835 test_accuracy_micro: 0.9431818127632141 auc: 0.9903475046157837 precision: 0.6363636255264282 recall 0.5606061220169067 f1 0.5878788232803345\n",
            "test_loss: 1.3785090446472168 test_accuracy_micro: 0.9772727489471436 auc: 0.9990347623825073 precision: 0.7272727489471436 recall 0.6666667461395264 f1 0.6909091472625732\n",
            "test_loss: 2.420691967010498 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.9090909957885742 recall 0.9090909957885742 f1 0.9090909957885742\n",
            "test_loss: 2.1196775436401367 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 1.428645372390747 test_accuracy_micro: 0.9431818127632141 auc: 0.9897435903549194 precision: 0.4545454680919647 recall 0.37878790497779846 f1 0.4060606062412262\n",
            "test_loss: 1.179577112197876 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.4545454680919647 recall 0.4545454680919647 f1 0.4545454680919647\n",
            "test_loss: 1.8715887069702148 test_accuracy_micro: 0.9659090638160706 auc: 0.9991319179534912 precision: 0.6363636255264282 recall 0.5530303120613098 f1 0.5870130062103271\n",
            "test_loss: 1.4789202213287354 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 0.9505385160446167 test_accuracy_micro: 0.9886363744735718 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 0.69525545835495 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.6363636255264282 recall 0.6363636255264282 f1 0.6363636255264282\n",
            "test_loss: 1.1953668594360352 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 1.2907272577285767 test_accuracy_micro: 0.9772727489471436 auc: 0.9958972930908203 precision: 0.6363636255264282 recall 0.575757622718811 f1 0.6000000238418579\n",
            "test_loss: 2.6956663131713867 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.9090909957885742 recall 0.9090909957885742 f1 0.9090909957885742\n",
            "test_loss: 1.4612150192260742 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.7272727489471436 recall 0.7272727489471436 f1 0.7272727489471436\n",
            "test_loss: 1.9173601865768433 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.8181818723678589 recall 0.8181818723678589 f1 0.8181818723678589\n",
            "test_loss: 2.050449848175049 test_accuracy_micro: 0.9659090638160706 auc: 0.9947916269302368 precision: 0.7272727489471436 recall 0.5909091234207153 f1 0.6363636255264282\n",
            "test_loss: 4.176353454589844 test_accuracy_micro: 0.9431818127632141 auc: 0.9661763906478882 precision: 0.5909091234207153 recall 0.6515152454376221 f1 0.5878788232803345\n",
            "test_loss: 3.5163543224334717 test_accuracy_micro: 0.8977272510528564 auc: 0.9353281855583191 precision: 0.5909091234207153 recall 0.5151515007019043 f1 0.5424242615699768\n",
            "test_loss: 0.17024940252304077 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.1818181872367859 recall 0.1818181872367859 f1 0.1818181872367859\n",
            "test_loss: 1.584837555885315 test_accuracy_micro: 1.0 auc: 1.0000001192092896 precision: 0.4545454680919647 recall 0.4545454680919647 f1 0.4545454680919647\n",
            "test_loss: 2.3456833362579346 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.4545454680919647 recall 0.4545454680919647 f1 0.4545454680919647\n",
            "test_loss: 1.6506283283233643 test_accuracy_micro: 1.0 auc: 1.0 precision: 0.3636363744735718 recall 0.3636363744735718 f1 0.3636363744735718\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m           auc           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9927492141723633    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m           f1            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6293193101882935    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6518596410751343    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         recall          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.618973970413208    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   test_accuracy_micro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9803718328475952    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.8453432321548462    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            auc            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9927492141723633     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            f1             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6293193101882935     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6518596410751343     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          recall           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.618973970413208     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_micro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9803718328475952     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.8453432321548462     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 1.8453432321548462,\n",
              "  'test_accuracy_micro': 0.9803718328475952,\n",
              "  'auc': 0.9927492141723633,\n",
              "  'precision': 0.6518596410751343,\n",
              "  'recall': 0.618973970413208,\n",
              "  'f1': 0.6293193101882935}]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "trainer.test(model=classification_module, datamodule=data_module, verbose=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zF1yCYSsRjY5",
        "POxqkd5aRrmt",
        "BtFuX8SuCGr-"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eec9e9f1996457e9f20ef6a937f0813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68ca154fd0e64be89deafcd1b9a58d51",
              "IPY_MODEL_3ef3e329ac484f508d7d6768337d9f0a",
              "IPY_MODEL_93839f48bec34db887962c1a7cdb0035"
            ],
            "layout": "IPY_MODEL_fee7f2d5041e4ac2b389396275913ede"
          }
        },
        "68ca154fd0e64be89deafcd1b9a58d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90063137eb8f4af89dd3759a62929d6a",
            "placeholder": "​",
            "style": "IPY_MODEL_521c802bce4740a38eea286d87e74087",
            "value": "Testing DataLoader 0: "
          }
        },
        "3ef3e329ac484f508d7d6768337d9f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc8079d9f0fe45c890979a9d7c506cb7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44960fd5b0784b7696e3de0aa4f2ca1f",
            "value": 1
          }
        },
        "93839f48bec34db887962c1a7cdb0035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d43b15a5c3040cfa49ba6fc2fb938e7",
            "placeholder": "​",
            "style": "IPY_MODEL_404856f5f586439fa7ae55ffef553012",
            "value": " 40/? [03:57&lt;00:00,  0.17it/s]"
          }
        },
        "fee7f2d5041e4ac2b389396275913ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "90063137eb8f4af89dd3759a62929d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521c802bce4740a38eea286d87e74087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc8079d9f0fe45c890979a9d7c506cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44960fd5b0784b7696e3de0aa4f2ca1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d43b15a5c3040cfa49ba6fc2fb938e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404856f5f586439fa7ae55ffef553012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}