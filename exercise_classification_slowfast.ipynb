{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpEPGNddtccp"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "This is a Google Colab Notebook which trains a PySlowFast model (using pytorchvideo) on a dataset of videos. The dataset's directory should have 3 sub-directories, one for \"train\", \"val\", and \"test\". In each of these directories should be more directories, named with the dataset's classes. Then, within each of these directories should be videos, whose label corresponds to the directory they are placed in (e.g. dataset->train->archery->archery_1.mph).\n",
        "\n",
        "The \"Set-up\" section is used for downloading necessary packages and intiliazing variables. Take a look at the variables (such as file paths), and change them accordingly.\n",
        "\n",
        "The \"Focal Loss and Dataset Distribution\" section has code that finds distribution of the used dataset and uses the information to set up Focal Loss.\n",
        "\n",
        "The \"Class Creation for Dataloaders\" section creates the classes/objects necessary for creating and using dataloaders later in the process. Within this section is a commented code block which would change the transform function to one of a resnet. Uncommenting this code block, and changing the model to \"make_resnet\" in \"Model Creation\" would use a resnet50 instead.\n",
        "\n",
        "The \"Model Creation\" section creates the object that is used as the Lightning Module for the model.\n",
        "\n",
        "The \"Training\" section trains the model using the Trainer from Pytorch Lightning.\n",
        "\n",
        "The \"Testing\" section tests the model using Trainer.test and a custom script that tests every file in the testing dataset.\n",
        "\n",
        "The \"TSNE\" section makes a TSNE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1yCYSsRjY5"
      },
      "source": [
        "# Set-up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP8OZwTSRk0W",
        "outputId": "bf473fd7-2df5-4180-ea0e-97487310aad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsUoGFOVQlw2"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0KrecHPgICZ",
        "outputId": "4ca296fa-284c-4e11-dd59-289e97801708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore (from pytorchvideo)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av (from pytorchvideo)\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting iopath (from pytorchvideo)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.5.0)\n",
            "Collecting portalocker (from iopath->pytorchvideo)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=b6ec3f1d7cc2d92fd662029a2ee1b2ed75cbb8381e184962bff6c2789d81e58a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=3eb28bc8f6fa8051dbdbcb0eb1037f6d599f7a29b99ad65994e8c6f9568b332a\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=7cf7cc492d2425381a5967ede4217b411e6aa64d910df3c68a7f099d117b3b21\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-11.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.8.2 pytorchvideo-0.1.5 yacs-0.1.8\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.10.0 pytorch_lightning-2.1.3 torchmetrics-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorchvideo\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ykMDjE9PI-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def9988-c2d4-4690-a728-d3b1c9d987de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pytorch_lightning\n",
        "import pytorchvideo.data\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import pytorchvideo\n",
        "import pytorchvideo.transforms\n",
        "from torch.nn.functional import softmax\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "import sklearn\n",
        "#from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxNxaoChPXVV"
      },
      "outputs": [],
      "source": [
        "input_dir = '/content/drive/MyDrive/Research/Muscle Video/Datasets/split_workout_videos_v1'\n",
        "model_name = \"slowfast_r50\"\n",
        "checkpoint_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained' + '/' + model_name\n",
        "\n",
        "num_classes=16\n",
        "batch_size = 8\n",
        "num_workers = 8\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pretrained=True\n",
        "learning_rate=0.0001\n",
        "dropout_rate = 0.6\n",
        "gamma = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r7rgq6C8XWS",
        "outputId": "086bbcf3-7264-4dae-ddd7-6d06b36ccf91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWfh_LFES6Ew"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "pred_to_class = {\n",
        "    0: \"barbell biceps curl\",\n",
        "    1: \"bench press\",\n",
        "    2: \"chest fly machine\",\n",
        "    3: \"deadlift\",\n",
        "    4: \"decline bench press\",\n",
        "    5: \"hammer curl\",\n",
        "    6: \"hip thrust\",\n",
        "    7: \"incline bench press\",\n",
        "    8: \"lat pulldown\",\n",
        "    9: \"lateral raise\",\n",
        "    10: \"leg extension\",\n",
        "    11: \"leg raises\",\n",
        "    12: \"plank\",\n",
        "    13: \"pull Up\",\n",
        "    14: \"push-up\",\n",
        "    15: \"romanian deadlift\",\n",
        "    16: \"russian twist\",\n",
        "    17: \"shoulder press\",\n",
        "    18: \"squat\",\n",
        "    19: \"t bar row\",\n",
        "    20: \"tricep Pushdown\",\n",
        "    21: \"tricep dips\"\n",
        "}\n",
        "'''\n",
        "pred_to_class = {\n",
        "    0: \"bench press\",\n",
        "    1: \"bicep curl\",\n",
        "    2: \"chest fly machine\",\n",
        "    3: \"deadlift\",\n",
        "    4: \"hip thrust\",\n",
        "    5: \"lat pulling\",\n",
        "    6: \"lateral raise\",\n",
        "    7: \"leg extension\",\n",
        "    8: \"leg raises\",\n",
        "    9: \"push-up\",\n",
        "    10: \"russian twist\",\n",
        "    11: \"shoulder press\",\n",
        "    12: \"squat\",\n",
        "    13: \"t bar row\",\n",
        "    14: \"tricep Pushdown\",\n",
        "    15: \"tricep dips\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Focal Loss and Dataset Distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "vUCYRaUcwEN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = input_dir"
      ],
      "metadata": {
        "id": "zkyHFtPZwCi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_values = {}\n",
        "test_class_values = {}\n",
        "val_class_values = {}\n",
        "for split in os.listdir(dataset):\n",
        "  new_path = os.path.join(dataset, split)\n",
        "  for classname in os.listdir(new_path):\n",
        "    class_path = os.path.join(new_path, classname)\n",
        "    num_samples = len(os.listdir(class_path))\n",
        "    print(\"In the\", split, \"split, for the\", classname, \"class, there are\", num_samples, \"samples.\")\n",
        "    if split==\"train\":\n",
        "      train_class_values[classname] = num_samples\n",
        "    if split==\"test\":\n",
        "      test_class_values[classname] = num_samples\n",
        "    if split==\"val\":\n",
        "      val_class_values[classname] = num_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgUMwMFIwU-D",
        "outputId": "de9a929c-dc35-49a7-9075-488da6f3ddd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the train split, for the bench press class, there are 183 samples.\n",
            "In the train split, for the lateral raise class, there are 67 samples.\n",
            "In the train split, for the chest fly machine class, there are 49 samples.\n",
            "In the train split, for the hip thrust class, there are 55 samples.\n",
            "In the train split, for the deadlift class, there are 129 samples.\n",
            "In the train split, for the leg extension class, there are 53 samples.\n",
            "In the train split, for the push-up class, there are 78 samples.\n",
            "In the train split, for the squat class, there are 77 samples.\n",
            "In the train split, for the leg raises class, there are 43 samples.\n",
            "In the train split, for the t bar row class, there are 60 samples.\n",
            "In the train split, for the shoulder press class, there are 42 samples.\n",
            "In the train split, for the russian twist class, there are 53 samples.\n",
            "In the train split, for the tricep Pushdown class, there are 40 samples.\n",
            "In the train split, for the tricep dips class, there are 72 samples.\n",
            "In the train split, for the bicep curl class, there are 106 samples.\n",
            "In the train split, for the lat pulling class, there are 111 samples.\n",
            "In the test split, for the bench press class, there are 37 samples.\n",
            "In the test split, for the lateral raise class, there are 33 samples.\n",
            "In the test split, for the chest fly machine class, there are 18 samples.\n",
            "In the test split, for the hip thrust class, there are 15 samples.\n",
            "In the test split, for the deadlift class, there are 49 samples.\n",
            "In the test split, for the leg extension class, there are 20 samples.\n",
            "In the test split, for the push-up class, there are 38 samples.\n",
            "In the test split, for the squat class, there are 39 samples.\n",
            "In the test split, for the leg raises class, there are 17 samples.\n",
            "In the test split, for the t bar row class, there are 28 samples.\n",
            "In the test split, for the shoulder press class, there are 23 samples.\n",
            "In the test split, for the russian twist class, there are 19 samples.\n",
            "In the test split, for the tricep Pushdown class, there are 17 samples.\n",
            "In the test split, for the tricep dips class, there are 25 samples.\n",
            "In the test split, for the bicep curl class, there are 30 samples.\n",
            "In the test split, for the lat pulling class, there are 34 samples.\n",
            "In the val split, for the bench press class, there are 37 samples.\n",
            "In the val split, for the lateral raise class, there are 33 samples.\n",
            "In the val split, for the chest fly machine class, there are 18 samples.\n",
            "In the val split, for the hip thrust class, there are 16 samples.\n",
            "In the val split, for the deadlift class, there are 49 samples.\n",
            "In the val split, for the leg extension class, there are 20 samples.\n",
            "In the val split, for the push-up class, there are 38 samples.\n",
            "In the val split, for the squat class, there are 39 samples.\n",
            "In the val split, for the leg raises class, there are 17 samples.\n",
            "In the val split, for the t bar row class, there are 28 samples.\n",
            "In the val split, for the shoulder press class, there are 23 samples.\n",
            "In the val split, for the russian twist class, there are 19 samples.\n",
            "In the val split, for the tricep Pushdown class, there are 17 samples.\n",
            "In the val split, for the tricep dips class, there are 25 samples.\n",
            "In the val split, for the bicep curl class, there are 30 samples.\n",
            "In the val split, for the lat pulling class, there are 34 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_values"
      ],
      "metadata": {
        "id": "J52qFuM_wW87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8210175-12ee-4a9d-e00d-9854cfdb7d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bench press': 183,\n",
              " 'lateral raise': 67,\n",
              " 'chest fly machine': 49,\n",
              " 'hip thrust': 55,\n",
              " 'deadlift': 129,\n",
              " 'leg extension': 53,\n",
              " 'push-up': 78,\n",
              " 'squat': 77,\n",
              " 'leg raises': 43,\n",
              " 't bar row': 60,\n",
              " 'shoulder press': 42,\n",
              " 'russian twist': 53,\n",
              " 'tricep Pushdown': 40,\n",
              " 'tricep dips': 72,\n",
              " 'bicep curl': 106,\n",
              " 'lat pulling': 111}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiXlkXzk2Tj9",
        "outputId": "b12fdee3-87ae-423e-adaf-2bd9755d92b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bench press': 37,\n",
              " 'lateral raise': 33,\n",
              " 'chest fly machine': 18,\n",
              " 'hip thrust': 15,\n",
              " 'deadlift': 49,\n",
              " 'leg extension': 20,\n",
              " 'push-up': 38,\n",
              " 'squat': 39,\n",
              " 'leg raises': 17,\n",
              " 't bar row': 28,\n",
              " 'shoulder press': 23,\n",
              " 'russian twist': 19,\n",
              " 'tricep Pushdown': 17,\n",
              " 'tricep dips': 25,\n",
              " 'bicep curl': 30,\n",
              " 'lat pulling': 34}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_class_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m0eTKZlWHrO",
        "outputId": "2a4c6c3b-a40d-4711-f125-6309a7d6a421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bench press': 37,\n",
              " 'lateral raise': 33,\n",
              " 'chest fly machine': 18,\n",
              " 'hip thrust': 16,\n",
              " 'deadlift': 49,\n",
              " 'leg extension': 20,\n",
              " 'push-up': 38,\n",
              " 'squat': 39,\n",
              " 'leg raises': 17,\n",
              " 't bar row': 28,\n",
              " 'shoulder press': 23,\n",
              " 'russian twist': 19,\n",
              " 'tricep Pushdown': 17,\n",
              " 'tricep dips': 25,\n",
              " 'bicep curl': 30,\n",
              " 'lat pulling': 34}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label = {\n",
        "    0: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    1: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    3: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
        "    4: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    5: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    6: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    7: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    9: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
        "    10: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
        "    11: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    12: [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "    13: [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    14: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    15: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "}\n",
        "\n",
        "class_to_id = dict([(value, key) for key, value in pred_to_class.items()])"
      ],
      "metadata": {
        "id": "ZCRgEttPWGhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "for classname in train_class_values:\n",
        "  id = class_to_id[classname]\n",
        "  label_thing = id_to_label[id].copy()\n",
        "\n",
        "  for x in range(0, 11):\n",
        "    label_thing[x]= label_thing[x] * train_class_values[classname]\n",
        "    train_label_count[x] += label_thing[x]"
      ],
      "metadata": {
        "id": "8XlW1nuBXUFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Zkquj-YHCU",
        "outputId": "3d14b0ef-0d0c-48a0-aaee-dc21dcd5715d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[373.0, 171.0, 277.0, 130.0, 261.0, 370.0, 96.0, 53.0, 310.0, 129.0, 206.0]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_label_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "for classname in val_class_values:\n",
        "  id = class_to_id[classname]\n",
        "  label_thing = id_to_label[id].copy()\n",
        "\n",
        "  for x in range(0, 11):\n",
        "    label_thing[x]= label_thing[x] * val_class_values[classname]\n",
        "    val_label_count[x] += label_thing[x]"
      ],
      "metadata": {
        "id": "0ucP-rl8Yaua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_label_count"
      ],
      "metadata": {
        "id": "GcHcfa9xYpLT",
        "outputId": "36a4dc12-57b8-42a8-d781-88f94b8d03eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[117.0, 62.0, 92.0, 59.0, 104.0, 131.0, 36.0, 19.0, 93.0, 49.0, 88.0]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_label_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "for classname in test_class_values:\n",
        "  id = class_to_id[classname]\n",
        "  label_thing = id_to_label[id].copy()\n",
        "\n",
        "  for x in range(0, 11):\n",
        "    label_thing[x]= label_thing[x] * test_class_values[classname]\n",
        "    test_label_count[x] += label_thing[x]"
      ],
      "metadata": {
        "id": "BI2EIaTjYlzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label_count"
      ],
      "metadata": {
        "id": "n7o0lgctYqme",
        "outputId": "08feaa3d-ac6d-4d26-fa25-f3829be5d192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[117.0, 62.0, 92.0, 59.0, 103.0, 131.0, 36.0, 19.0, 93.0, 49.0, 88.0]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "for classname in train_class_values:\n",
        "  total = total + train_class_values[classname]"
      ],
      "metadata": {
        "id": "mk3hYAv8wYRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOJ-pjYwZcI",
        "outputId": "041d4200-2578-47ce-c334-dde77c919270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1218"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_test = 0\n",
        "for classname in test_class_values:\n",
        "  total_test = total_test + test_class_values[classname]"
      ],
      "metadata": {
        "id": "ImR0pmXb2hcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJcfnMzQ2n3j",
        "outputId": "183ba716-1136-487a-f983-4f729b09f51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_val = 0\n",
        "for classname in val_class_values:\n",
        "  total_val += val_class_values[classname]"
      ],
      "metadata": {
        "id": "VTK4BGvNWWKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQZkHKkOWcCj",
        "outputId": "575d1d61-87b2-4cb7-8241-c7c96cfe10b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "443"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {}\n",
        "for classname in train_class_values:\n",
        "  weight = 1 / (train_class_values[classname] / total)\n",
        "  class_weights[classname] = weight"
      ],
      "metadata": {
        "id": "pIvbIiLYwarE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEgZyxCSwcYr",
        "outputId": "78c957b0-7fa9-4fad-f6f4-24459e0b1a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bench press': 6.655737704918033,\n",
              " 'lateral raise': 18.17910447761194,\n",
              " 'chest fly machine': 24.857142857142858,\n",
              " 'hip thrust': 22.145454545454545,\n",
              " 'deadlift': 9.44186046511628,\n",
              " 'leg extension': 22.981132075471695,\n",
              " 'push-up': 15.615384615384615,\n",
              " 'squat': 15.818181818181817,\n",
              " 'leg raises': 28.325581395348838,\n",
              " 't bar row': 20.3,\n",
              " 'shoulder press': 29.0,\n",
              " 'russian twist': 22.981132075471695,\n",
              " 'tricep Pushdown': 30.45,\n",
              " 'tricep dips': 16.916666666666668,\n",
              " 'bicep curl': 11.490566037735848,\n",
              " 'lat pulling': 10.972972972972974}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_int = {}\n",
        "for i in pred_to_class:\n",
        "  class_to_int[pred_to_class[i]]=i"
      ],
      "metadata": {
        "id": "b6sRH_5X-bkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6TfAeR7-pVc",
        "outputId": "bba69561-3dfc-41df-fa75-3849843dff91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bench press': 0,\n",
              " 'bicep curl': 1,\n",
              " 'chest fly machine': 2,\n",
              " 'deadlift': 3,\n",
              " 'hip thrust': 4,\n",
              " 'lat pulling': 5,\n",
              " 'lateral raise': 6,\n",
              " 'leg extension': 7,\n",
              " 'leg raises': 8,\n",
              " 'push-up': 9,\n",
              " 'russian twist': 10,\n",
              " 'shoulder press': 11,\n",
              " 'squat': 12,\n",
              " 't bar row': 13,\n",
              " 'tricep Pushdown': 14,\n",
              " 'tricep dips': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_weights = [None] * num_classes\n",
        "for i in class_weights:\n",
        "  list_weights[class_to_int[i]] = class_weights[i]\n",
        "list_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33QoWwH79y-i",
        "outputId": "39ddcf0c-03c8-488f-ddaa-00b144e542d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.655737704918033,\n",
              " 11.490566037735848,\n",
              " 24.857142857142858,\n",
              " 9.44186046511628,\n",
              " 22.145454545454545,\n",
              " 10.972972972972974,\n",
              " 18.17910447761194,\n",
              " 22.981132075471695,\n",
              " 28.325581395348838,\n",
              " 15.615384615384615,\n",
              " 22.981132075471695,\n",
              " 29.0,\n",
              " 15.818181818181817,\n",
              " 20.3,\n",
              " 30.45,\n",
              " 16.916666666666668]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "wCDEEXJHweMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops.focal_loss import sigmoid_focal_loss"
      ],
      "metadata": {
        "id": "PL_p8LUYAmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#focal_loss = sigmoid_focal_loss(alpha=class_weights, gamma=2)"
      ],
      "metadata": {
        "id": "BOGvzyfnwwdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POxqkd5aRrmt"
      },
      "source": [
        "# Class Creation for Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0W233scPfx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410cc76f-b9ae-442d-82cc-3deaacd1ad2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom pytorchvideo.models.slowfast import create_slowfast\\n\\ndef make_slowfast():\\n    return create_slowfast(\\n        input_channels=(3, 3),\\n        model_depth=18,\\n        model_num_class=num_classes,\\n        norm=nn.BatchNorm3d,\\n        dropout_rate=dropout_rate,\\n    )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "def post_act(input):\n",
        "  return softmax(input, dim=1)\n",
        "\n",
        "'''\n",
        "from pytorchvideo.models.slowfast import create_slowfast\n",
        "\n",
        "def make_slowfast():\n",
        "    return create_slowfast(\n",
        "        input_channels=(3, 3),\n",
        "        model_depth=18,\n",
        "        model_num_class=num_classes,\n",
        "        norm=nn.BatchNorm3d,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de65-hoGR0ES"
      },
      "outputs": [],
      "source": [
        "class VideosDataModule(pytorch_lightning.LightningDataModule):\n",
        "\n",
        "    # Dataset configuration\n",
        "    _DATA_PATH = input_dir\n",
        "    _CLIP_DURATION = clip_duration  # Duration of sampled clip for each video\n",
        "    _BATCH_SIZE = batch_size\n",
        "    _NUM_WORKERS = num_workers  # Number of parallel processes fetching data\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        #Create the train partition from the list of video labels and video paths\n",
        "        train_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'train'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        #Create the validation partition from the list of video labels and video paths\n",
        "        val_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'val'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataset = pytorchvideo.data.labeled_video_dataset(\n",
        "            data_path=os.path.join(self._DATA_PATH, 'test'),\n",
        "            clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
        "            decode_audio=False,\n",
        "            transform=transform\n",
        "        )\n",
        "\n",
        "        return torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self._BATCH_SIZE,\n",
        "            num_workers=self._NUM_WORKERS,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUKUCHxR5g6"
      },
      "source": [
        "# Model Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW0o6pPoR-w4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "mca1 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=1)\n",
        "mca5 = MulticlassAccuracy(num_classes=num_classes, average='micro', top_k=5)\n",
        "\n",
        "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
        "        self.model.to(device)\n",
        "        self.model.blocks[6].proj = nn.Linear(in_features=2304, out_features=16, bias=True)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
        "        # format provided by the dataset\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "\n",
        "        # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
        "        # by PyTorchLightning after being returned from this method.\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # Log the train loss to Tensorboard\n",
        "        self.log(\"train_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"train_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"train_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        #last_layer = self.model.blocks[6].proj\n",
        "        #embeddings = []\n",
        "\n",
        "        print(\"train_loss:\", loss.item(), \"train_accuracy_top_1:\", acc1.item(), \"train_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"val_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"val_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        print(\"val_loss:\", loss.item(), \"val_accuracy_top_1:\", acc1.item(), \"val_accuracy_top_5:\", acc5.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
        "        usually useful for training video models.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        y_hat = self.model(batch[\"video\"])\n",
        "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
        "\n",
        "        # logs metrics for each testing_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss.item(), batch_size=batch_size)\n",
        "\n",
        "        acc1 = mca1(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        acc5 = mca5(y_hat.cpu(), batch[\"label\"].cpu())\n",
        "        self.log(\"test_accuracy_top_1\", acc1.item(), batch_size=batch_size)\n",
        "        self.log(\"test_accuracy_top_5\", acc5.item(), batch_size=batch_size)\n",
        "\n",
        "        softed = post_act(y_hat)\n",
        "        auc = roc_auc_score(y_true=batch[\"label\"].cpu(), y_score=softed.cpu(), multi_class='ovr', average='micro', labels=np.arange(num_classes))\n",
        "        self.log(\"auc\", auc, batch_size=batch_size)\n",
        "\n",
        "        pred_classes = []\n",
        "        for x in softed:\n",
        "          class_index = x.topk(k=1).indices\n",
        "          class_index = class_index[0]\n",
        "          pred_classes.append(class_index)\n",
        "        pred_classes = torch.Tensor(pred_classes)\n",
        "\n",
        "        rpf1 = precision_recall_fscore_support(y_true=batch[\"label\"].cpu(), y_pred=pred_classes.cpu(), beta=1, labels=np.arange(num_classes), average='macro', zero_division=1)\n",
        "        precision = rpf1[0]\n",
        "        recall = rpf1[1]\n",
        "        f1 = rpf1[2]\n",
        "        self.log(\"precision\", precision, batch_size=batch_size)\n",
        "        self.log(\"recall\", recall, batch_size=batch_size)\n",
        "        self.log(\"f1\", f1, batch_size=batch_size)\n",
        "\n",
        "        print(\"test_loss:\", loss.item(), \"test_accuracy_top_1:\", acc1.item(), \"test_accuracy_top_5:\", acc5.item(), \"auc:\", auc, \"precision:\", precision, \"recall:\", recall, \"f1:\", f1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5dqGI2sSaZz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggfgr6aSww4a"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping_callbacks = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10, verbose=True, mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yBLmhEQTLbG",
        "outputId": "72e8a2ae-034a-4269-fa64-6e4e1a76f26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "classification_module = VideoClassificationLightningModule()\n",
        "data_module = VideosDataModule()\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    default_root_dir=checkpoint_path,\n",
        "    max_epochs=30,\n",
        "    accelerator=\"auto\",\n",
        "    devices=\"auto\",\n",
        "    strategy='auto',\n",
        "    enable_checkpointing=True,\n",
        "    logger=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEXoz1KX7_C7"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldyTiRKsvSyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "outputId": "a23e9735-3345-41a0-a720-bc02e5092b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-443819fa368d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: configuring model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# restore modules after setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore_training_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoClassificationLightningModule:\n\tUnexpected key(s) in state_dict: \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_a.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_b.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.conv_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.weight\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.bias\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_mean\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.running_var\", \"model.blocks.3.multipathway_blocks.0.res_blocks.6.branch2.norm_c.num_batches_tracked\", \"model.blocks.3.multipathway_blocks.0.res_block...\n\tsize mismatch for model.blocks.0.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([16, 8, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 8, 7, 1, 1]).\n\tsize mismatch for model.blocks.1.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([64, 32, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 7, 1, 1]).\n\tsize mismatch for model.blocks.2.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([128, 64, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 7, 1, 1]).\n\tsize mismatch for model.blocks.3.multipathway_fusion.conv_fast_to_slow.weight: copying a param with shape torch.Size([256, 128, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 7, 1, 1])."
          ]
        }
      ],
      "source": [
        "trainer.fit(classification_module, data_module, ckpt_path='/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_5/checkpoints/epoch=28-step=3754.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csUFcsFoKft"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MzWhWAq8qJk"
      },
      "outputs": [],
      "source": [
        "best_checkpoint_path = '/content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r50/lightning_logs/version_4/checkpoints/epoch=4-step=640.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_QYVE-AUbhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f1cd609d4514cb5ab030a29f8132c1b",
            "7df5670482394cbd92410b1fcf9fd1f8",
            "f88ea33e6c5849a3a9392650375ec83c",
            "647d2c249b3e4deda3ba2f3a0dcc8e7b",
            "88765b6572414f958e7337541f7aa0e1",
            "43ea3577af0f44db870dff2f1220d51b",
            "59c4213ed14e448fa9ceb5043b554659",
            "a1ae87568d234535a7372ed76d436b5c",
            "9137bc3cf5a1412da9637f395d5cb076",
            "d2cb445200fc4224b87a7ca908f3e2fe",
            "faf9971d5c924fe9a555c049919724bb"
          ]
        },
        "outputId": "347db6fa-0a4d-49e6-ec3a-00789b5ba490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_3/checkpoints/epoch=49-step=6442.ckpt\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from the checkpoint at /content/drive/MyDrive/Research/Muscle Video/Checkpoints/Split/Pretrained/slowfast_r101/lightning_logs/version_3/checkpoints/epoch=49-step=6442.ckpt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f1cd609d4514cb5ab030a29f8132c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 0.09196822345256805 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.11874991655349731 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.06732739508152008 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.04840409755706787 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.08243775367736816 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.2779752016067505 test_accuracy_top_1: 0.875 test_accuracy_top_5: 1.0 auc: 0.9979166666666667 precision: 0.9375 recall: 0.9375 f1: 0.875\n",
            "test_loss: 0.012451879680156708 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.01525782234966755 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.16348227858543396 test_accuracy_top_1: 0.875 test_accuracy_top_5: 1.0 auc: 0.9989583333333334 precision: 0.9375 recall: 0.9375 f1: 0.875\n",
            "test_loss: 0.07907923310995102 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.139902725815773 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.11731301248073578 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.7641542553901672 test_accuracy_top_1: 0.75 test_accuracy_top_5: 1.0 auc: 0.9864583333333333 precision: 0.9583333333333333 recall: 0.9583333333333333 f1: 0.9375\n",
            "test_loss: 0.011265124194324017 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.020573263987898827 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.09919366240501404 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.061297979205846786 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.1435963213443756 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.01294437050819397 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.13149459660053253 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.09232860803604126 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.23790965974330902 test_accuracy_top_1: 0.875 test_accuracy_top_5: 1.0 auc: 0.996875 precision: 0.9791666666666666 recall: 0.96875 f1: 0.9666666666666666\n",
            "test_loss: 0.06474314630031586 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.8122511506080627 test_accuracy_top_1: 0.875 test_accuracy_top_5: 0.875 auc: 0.9812500000000001 precision: 0.9375 recall: 0.9375 f1: 0.875\n",
            "test_loss: 0.07836497575044632 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.10815166682004929 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.09372906386852264 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.04282364994287491 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.2982925772666931 test_accuracy_top_1: 0.875 test_accuracy_top_5: 1.0 auc: 0.9968750000000001 precision: 0.96875 recall: 0.9791666666666666 f1: 0.9666666666666666\n",
            "test_loss: 0.01742173545062542 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.10797880589962006 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.02047337219119072 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.059886664152145386 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.21076226234436035 test_accuracy_top_1: 0.875 test_accuracy_top_5: 1.0 auc: 0.9989583333333334 precision: 0.9375 recall: 0.9375 f1: 0.875\n",
            "test_loss: 0.06116599962115288 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.052368588745594025 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.018046235665678978 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.09907460957765579 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.13122405111789703 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.004603380803018808 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.00031479846802540123 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.0028018122538924217 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 2.5748875486897305e-05 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.004658559802919626 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.0008516260422766209 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.0025118011981248856 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 0.02150888368487358 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n",
            "test_loss: 7.760223525110632e-05 test_accuracy_top_1: 1.0 test_accuracy_top_5: 1.0 auc: 1.0 precision: 1.0 recall: 1.0 f1: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m           auc           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9991102814674377    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m           f1            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9868924021720886    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9928385615348816    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         recall          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9928385615348816    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   test_accuracy_top_1   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9791666865348816    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m   test_accuracy_top_5   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9973958134651184    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10631709545850754   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            auc            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9991102814674377     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">            f1             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9868924021720886     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9928385615348816     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          recall           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9928385615348816     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_top_1    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9791666865348816     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_accuracy_top_5    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9973958134651184     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10631709545850754    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_loss': 0.10631709545850754,\n",
              "  'test_accuracy_top_1': 0.9791666865348816,\n",
              "  'test_accuracy_top_5': 0.9973958134651184,\n",
              "  'auc': 0.9991102814674377,\n",
              "  'precision': 0.9928385615348816,\n",
              "  'recall': 0.9928385615348816,\n",
              "  'f1': 0.9868924021720886}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "trainer.test(model=classification_module, datamodule=data_module, ckpt_path=best_checkpoint_path, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TSNE"
      ],
      "metadata": {
        "id": "BtFuX8SuCGr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TSNE_model = VideoClassificationLightningModule.load_from_checkpoint(best_checkpoint_path)\n",
        "TSNE_model.to('cpu')\n",
        "TSNE_model.model.blocks.pop(6)\n",
        "TSNE_model.eval()\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULEPmSyTCILk",
        "outputId": "28cc5c55-dfc2-4986-b5e1-ec664cae2ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RbmJ1rHPCLHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 10\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "7myJZnrNCMJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "features = None"
      ],
      "metadata": {
        "id": "W_oirFUNCOJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thingy = data_module.test_dataloader()\n",
        "for batch in thingy:\n",
        "  x = batch[\"video\"]\n",
        "  labels += batch[\"label\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = TSNE_model(x)\n",
        "    output = torch.flatten(output, 1)\n",
        "\n",
        "  current_features = output.cpu().numpy()\n",
        "\n",
        "  if features is not None:\n",
        "    features = np.concatenate((features, current_features))\n",
        "  else:\n",
        "    features = current_features"
      ],
      "metadata": {
        "id": "zLsYVEWECPrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2).fit_transform(features)"
      ],
      "metadata": {
        "id": "RnJeizXECSW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale and move the coordinates so they fit [0; 1] range\n",
        "def scale_to_01_range(x):\n",
        "    # compute the distribution range\n",
        "    value_range = (np.max(x) - np.min(x))\n",
        "\n",
        "    # move the distribution so that it starts from zero\n",
        "    # by extracting the minimal value from all its values\n",
        "    starts_from_zero = x - np.min(x)\n",
        "\n",
        "    # make the distribution fit [0; 1] by dividing by its range\n",
        "    return starts_from_zero / value_range\n",
        "\n",
        "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
        "tx = tsne[:, 0]\n",
        "ty = tsne[:, 1]\n",
        "\n",
        "tx = scale_to_01_range(tx)\n",
        "ty = scale_to_01_range(ty)"
      ],
      "metadata": {
        "id": "5PXEvxJlCUJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors_per_class = {\n",
        "    0: [254, 202, 87],\n",
        "    1: [255, 107, 107],\n",
        "    2: [10, 189, 227],\n",
        "    3: [255, 159, 243],\n",
        "    4: [16, 172, 132],\n",
        "    5: [128, 80, 128],\n",
        "    6: [87, 101, 116],\n",
        "    7: [52, 31, 151],\n",
        "    8: [0, 0, 0],\n",
        "    9: [100, 100, 255],\n",
        "    10: [128, 109, 84],\n",
        "    11: [15, 245, 218],\n",
        "    12: [242, 10, 21],\n",
        "    13: [242, 10, 149],\n",
        "    14: [40, 242, 0],\n",
        "    15: [255, 153, 10]\n",
        "}"
      ],
      "metadata": {
        "id": "QVF3CvuOCVZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a matplotlib plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# for every class, we'll add a scatter plot separately\n",
        "for label in colors_per_class:\n",
        "    # find the samples of the current class in the data\n",
        "    indices = [i for i, l in enumerate(labels) if l == label]\n",
        "\n",
        "    # extract the coordinates of the points of this class only\n",
        "    current_tx = np.take(tx, indices)\n",
        "    current_ty = np.take(ty, indices)\n",
        "\n",
        "    # convert the class color to matplotlib format\n",
        "    color = np.array(colors_per_class[label], dtype=np.float) / 255\n",
        "\n",
        "    # add a scatter plot with the corresponding color and label\n",
        "    ax.scatter(current_tx, current_ty, c=color, label=label, s=15)\n",
        "\n",
        "# build a legend using the labels we set previously\n",
        "ax.legend(loc='best')\n",
        "\n",
        "# finally, show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "7rIVvzH-CWkq",
        "outputId": "e03cd092-21dc-4173-d3a1-acbda46ac17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-885392fecd81>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  color = np.array(colors_per_class[label], dtype=np.float) / 255\n",
            "<ipython-input-52-885392fecd81>:18: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  ax.scatter(current_tx, current_ty, c=color, label=label, s=15)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zF1yCYSsRjY5",
        "vUCYRaUcwEN2",
        "POxqkd5aRrmt",
        "8hUKUCHxR5g6",
        "z5dqGI2sSaZz",
        "8csUFcsFoKft",
        "BtFuX8SuCGr-"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f1cd609d4514cb5ab030a29f8132c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7df5670482394cbd92410b1fcf9fd1f8",
              "IPY_MODEL_f88ea33e6c5849a3a9392650375ec83c",
              "IPY_MODEL_647d2c249b3e4deda3ba2f3a0dcc8e7b"
            ],
            "layout": "IPY_MODEL_88765b6572414f958e7337541f7aa0e1"
          }
        },
        "7df5670482394cbd92410b1fcf9fd1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ea3577af0f44db870dff2f1220d51b",
            "placeholder": "​",
            "style": "IPY_MODEL_59c4213ed14e448fa9ceb5043b554659",
            "value": "Testing DataLoader 0: "
          }
        },
        "f88ea33e6c5849a3a9392650375ec83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ae87568d234535a7372ed76d436b5c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9137bc3cf5a1412da9637f395d5cb076",
            "value": 1
          }
        },
        "647d2c249b3e4deda3ba2f3a0dcc8e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2cb445200fc4224b87a7ca908f3e2fe",
            "placeholder": "​",
            "style": "IPY_MODEL_faf9971d5c924fe9a555c049919724bb",
            "value": " 40/? [02:45&lt;00:00,  0.24it/s]"
          }
        },
        "88765b6572414f958e7337541f7aa0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "43ea3577af0f44db870dff2f1220d51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59c4213ed14e448fa9ceb5043b554659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1ae87568d234535a7372ed76d436b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9137bc3cf5a1412da9637f395d5cb076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2cb445200fc4224b87a7ca908f3e2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf9971d5c924fe9a555c049919724bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}